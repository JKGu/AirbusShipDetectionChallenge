{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os \nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport random\nimport csv\nimport cv2\nimport math\nimport PIL\nfrom collections import namedtuple, OrderedDict\nimport io\nfrom PIL import Image\nfrom collections import namedtuple, OrderedDict\n\n%matplotlib inline\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/airbus-ship-detection/' \nROOT_DIR = '/kaggle/working'\nos.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_v2_list = os.listdir(DATA_DIR + 'train_v2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DATA_DIR + \"train_ship_segmentations_v2.csv\")\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ShipCount'] = train_df.groupby('ImageId')['ImageId'].transform('count')\ntrain_df.loc[train_df['EncodedPixels'].isnull().values,'ShipCount'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = train_df.groupby('ShipCount').count()\ncount_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(count_df.index.values.tolist(), list(count_df['ImageId']))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Enhancement"},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleList = ['001aee007.jpg','001234638.jpg','001f04ca3.jpg','000d26c17.jpg']\nsampleImgList = []\nfor x in sampleList:\n    sampleImgList.append(mpimg.imread(DATA_DIR + 'train_v2/' + x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,10))\nfor i in range(len(sampleImgList)):\n    image_tmp = sampleImgList[i]\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.feature import canny\nfrom skimage.filters import scharr, unsharp_mask\nfrom skimage import exposure\nfrom skimage.color.adapt_rgb import adapt_rgb, each_channel, hsv_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,20))\nfor i in range(len(sampleImgList)):\n    image_tmp = sampleImgList[i]\n    ax = fig.add_subplot(2,4,i+1)\n    ax.imshow(image_tmp)\n    image_tmp = unsharp_mask(sampleImgList[i], radius=4, amount=2)\n    sampleImgList[i] = image_tmp\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoding the pixels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_to_pixels(rle_code):\n    '''\n    Transforms a RLE code string into a list of pixels of a (768, 768) canvas\n    '''\n    rle_code = [int(i) for i in rle_code.split()]\n#     pixels = [(pixel_position % 768, pixel_position // 768) \n#                  for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n#                  for pixel_position in range(start, start + length)]\n    \n    pixels = []\n    for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])):\n        for pixel_position in range(start, start + length):\n            pixels.append((pixel_position % 768, pixel_position // 768))\n            #if pixel_position < 0 or pixel_position > 768:\n                #print(pixel_position)\n    \n    return pixels\n\ndef get_all_masks(image_id):\n    ret = []\n    s = train_df[train_df['ImageId'] == image_id]['EncodedPixels']\n    if not s.isnull().values.any():\n        for x in s:\n            ret.append(rle_to_pixels(x))\n    return ret\n\ndef show_masks(image_id):\n    canvas = np.zeros((768, 768))\n    masks = get_all_masks(image_id)\n    for x in masks:\n       canvas[tuple(zip(*x))] = 1\n    return canvas\n\ndef find_bounding_box(pixels):\n    xmin = 767\n    xmax = 1\n    ymin = 767\n    ymax = 1\n    for p in pixels:\n        px = p[0]\n        py = p[1]\n        if px !=0 and py != 0 and px !=768 and py != 768:\n            if px < xmin:\n                xmin = px\n            if px > xmax:\n                xmax = px\n            if py < ymin:\n                ymin = py\n            if py > ymax:\n                ymax = py\n    return xmin, ymin, xmax, ymax\n\ndef get_all_boxes(image_id):\n    ret = []\n    masks = get_all_masks(image_id)\n    for x in masks:\n        ret.append(find_bounding_box(x))\n    return ret\n\ndef show_bounding_box(image_id):\n    canvas = np.array(PIL.Image.open(DATA_DIR + 'train_v2/' + image_id))\n    boxes = get_all_boxes(image_id)\n    for x in boxes:\n        xmin, ymin, xmax, ymax = x[0],x[1], x[2], x[3]\n        canvas[xmin][ymin : ymax] = [0,255,0]\n        canvas[xmax][ymin : ymax] = [0,255,0]\n        canvas[:,ymin][xmin : xmax] = [0,255,0]\n        canvas[:,ymax][xmin : xmax] = [0,255,0]\n    return canvas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,20))\nfor i in range(len(sampleList)): \n    image_tmp = show_masks(sampleList[i])\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)\n    image_tmp = show_bounding_box(sampleList[i])\n    ax = fig.add_subplot(2,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Object Detection API setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y tensorflow\n!pip uninstall -y tensorflow-gpu\n!pip uninstall -y tensorflow-estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change TF version\n!pip list | grep tensorflow\n!pip install tensorflow-gpu==1.15 #1.15\n# !pip uninstall -y tensorflow==2.2\n# !pip list | grep tensorflow\n!pip install tensorflow-estimator==1.15\n!pip list | grep tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip uninstall -y tensorflow-object-detection-api","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-object-detection-api==0.1.0 --no-dependencies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)\n!git clone https://github.com/tensorflow/models.git\n#!git clone https://github.com/tensorflow/models/archive/v2.2.0.zip\n# !wget  https://github.com/tensorflow/models/archive/v2.2.0.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get -y install protobuf-compiler\n!pip install Cython\n!pip install pillow\n!pip install lxml\n!pip install jupyter\n!pip install matplotlib\n!pip install tf_slim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR+\"/models/research/\")\n!protoc object_detection/protos/*.proto --python_out=.\n!export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\nos.environ['PYTHONPATH'] += ':/kaggle/working/models/research/:/kaggle/working/models/research/slim/:/kaggle/working/models'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test if set up is successful\n!python ./models/research/object_detection/builders/model_builder_test.py\n# !python object_detection/builders/model_builder_tf2_test.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LabelMap"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(ROOT_DIR+'/labelmap.pbtxt', 'w+') as the_file:\n    the_file.write('item\\n')\n    the_file.write('{\\n')\n    the_file.write('id :{}'.format(int(1)))\n    the_file.write('\\n')\n    the_file.write(\"name :'{0}'\".format('ship'))\n    the_file.write('\\n')\n    the_file.write('}\\n')\n    the_file.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate annotations"},{"metadata":{},"cell_type":"markdown","source":"## Generate xml files"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xml.etree.cElementTree as ET\ndef generate_xml(imageId):\n    annotation = ET.Element(\"annotation\")\n    ET.SubElement(annotation, \"folder\").text = \"train_v2\"\n    ET.SubElement(annotation, \"filename\").text = imageId\n    source = ET.SubElement(annotation, \"source\")\n    ET.SubElement(source, \"database\").text = \"Unknown\"\n    size = ET.SubElement(annotation, \"size\")\n    ET.SubElement(size, \"width\").text = \"768\"\n    ET.SubElement(size, \"height\").text = \"768\"\n    ET.SubElement(size, \"depth\").text = \"3\"\n    ET.SubElement(annotation, \"segmented\").text = \"0\"\n    \n    boxes = get_all_boxes(imageId)\n    for b in boxes:\n        object1 = ET.SubElement(annotation, \"object\")\n        ET.SubElement(object1, \"name\").text = \"ship\"\n        ET.SubElement(object1, \"name\").text = \"ship\"\n        ET.SubElement(object1, \"pose\").text = \"Unspecified\"\n        ET.SubElement(object1, \"truncated\").text = \"0\"\n        ET.SubElement(object1, \"difficult\").text = \"0\"\n        bndbox = ET.SubElement(object1, \"bndbox\")\n        xmin, ymin, xmax, ymax = b\n        ET.SubElement(bndbox, \"xmin\").text = str(xmin)\n        ET.SubElement(bndbox, \"ymin\").text = str(ymin)\n        ET.SubElement(bndbox, \"xmax\").text = str(xmax)\n        ET.SubElement(bndbox, \"ymax\").text = str(ymax)\n\n    tree = ET.ElementTree(annotation)\n    tree.write(\"test.xml\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate dataframe for TFRecords"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\n###############################################DEBUG\nfor x in train_df['ImageId'][:100]:\n    boxes = get_all_boxes(x)\n    for b in boxes:\n        xmin, ymin, xmax, ymax = b\n        data.append((x, 768, 768, 'ship', xmin, ymin, xmax, ymax))\ncolumns_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\nTFRcords_df = pd.DataFrame(data=data, columns=columns_name)\nTFRcords_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A naive way to split for code testing\n\ntrain_set = TFRcords_df[0:20]\nvalid_set = TFRcords_df[21:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install tf-nightly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append(\"..\")\nfrom models.research.object_detection.utils import dataset_util\nfrom models.research.object_detection.utils import label_map_util","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to group data and return the same\n# Group by imagefile name\ndef make_groups(df, field=None):\n    if field==None:\n        field = 'filename'\n\n    data = namedtuple('object', ['filename', 'info'])\n    grouped = df.groupby(field)\n\n    grouped_data = []\n    for filename, x in zip(grouped.groups.keys(), grouped.groups):\n        grouped_data.append(data(filename, grouped.get_group(x)))\n\n    return grouped_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a tf record sample\ndef create_tf_example(group, img_path, label_map_dict):\n    # Read the imagefile. This will be used in features later \n    with tf.io.gfile.GFile(os.path.join(img_path, '{}'.format(group.filename)), 'rb') as f:\n        img_file = f.read()\n\n        # Encode to bytes and read using PIL. Could be done directly too\n        encoded_img = io.BytesIO(img_file)\n        # Read the image using PIL\n        img = Image.open(encoded_img)\n        width, height = img.size\n\n      # Encode the name of the img file\n        filename = group.filename.encode('utf8')\n\n      # Define the format of the image file\n        img_format = b'jpg'   # The name will be in bytes\n\n\n      # Define the variables that you need as features\n        xmins = []\n        xmaxs = []\n        ymins = []\n        ymaxs = []\n        classes_text = []\n        classes = []\n\n      # Iterate over the namedtuple object\n        for index, row in group.info.iterrows():\n            xmins.append(row['xmin'] / width)   # store normalized values for bbox\n            xmaxs.append(row['xmax'] / width)\n            ymins.append(row['ymin'] / height)\n            ymaxs.append(row['ymax'] / height)\n            classes_text.append(row['class'].encode('utf8'))\n            classes.append(label_map_dict[row['class']])\n\n        tf_example = tf.train.Example(features=tf.train.Features(feature={\n          'image/height': dataset_util.int64_feature(height),\n          'image/width': dataset_util.int64_feature(width),\n          'image/filename': dataset_util.bytes_feature(filename),\n          'image/source_id': dataset_util.bytes_feature(filename),\n          'image/encoded': dataset_util.bytes_feature(img_file),\n          'image/format': dataset_util.bytes_feature(img_format),\n          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n          'image/object/class/label': dataset_util.int64_list_feature(classes),}))\n\n        return tf_example","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create TFRecord Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path where all the images are present\nimg_path = DATA_DIR + 'train_v2'\n# Label map\nlabel_map_dict = label_map_util.get_label_map_dict(ROOT_DIR + '/labelmap.pbtxt')\n\nwriter = tf.compat.v1.python_io.TFRecordWriter('./train.record')\n\n# create groups in the df. One image may contain several instances of an object hence the grouping thing\nimg_groups = make_groups(train_set, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for training data  created successfully\")\n\n\nwriter = tf.compat.v1.python_io.TFRecordWriter('./valid.record')\n# create groups \nimg_groups = make_groups(valid_set, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for validation data created successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!cp /kaggle/working/models/research/object_detection/samples/configs/ssd_inception_v2_coco.config /kaggle/working\n\n#!cp /kaggle/working/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config /kaggle/working\n!cp /kaggle/working/models/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz\n#!wget download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz\n#!wget download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!tar -xzf ssd_inception_v2_coco_2017_11_17.tar.gz\n#!tar -xzf faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz\n!tar -xzf faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mv ssd_inception_v2_coco_2017_11_17 mymodel\n#!mv faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8 mymodel\n!mv faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8 mymodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Cython\n!git clone https://github.com/pdollar/coco.git\nos.chdir('coco/PythonAPI')\n!make\n!make install\n!python setup.py install\nos.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configure the model config file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configures the .config automatically\n#fin = open(\"ssd_inception_v2_coco.config\", \"rt\")\n#fin = open(\"faster_rcnn_inception_v2_coco.config\", \"rt\")\nfin = open(\"faster_rcnn_inception_resnet_v2_atrous_coco.config\", \"rt\")\n\nfout = open(\"configfile.config\", \"wt\")\n\n\nfor line in fin:\n    if 'num_classes:' in line:\n        fout.write('\\t\\tnum_classes: 1\\n')\n    else:\n        line = line.replace('PATH_TO_BE_CONFIGURED/model.ckpt', '/kaggle/working/mymodel/model.ckpt')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100', '/kaggle/working/train.record')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt', '/kaggle/working/labelmap.pbtxt')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010','/kaggle/working/valid.record')\n        fout.write(line)\n\nfin.close()\nfout.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir checkpoints\n!cp /kaggle/working/models/research/object_detection/legacy/train.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/legacy/eval.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/export_inference_graph.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/model_main.py /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run the training\n# import tensorflow.compat.v2 as tf\n# !python train.py --logtostderr --train_dir=/kaggle/working/checkpoints/ --pipeline_config_path=/kaggle/working/configfile.config\n!python model_main.py  --logtostderr --train_dir=/kaggle/working/checkpoints/ --pipeline_config_path=/kaggle/working/configfile.config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python export_inference_graph.py --input_type image_tensor --pipeline_config_path /kaggle/working/configfile.config --trained_checkpoint_prefix ./checkpoints/model.ckpt-25823 --output_directory ./fine_tuned_model","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}